{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing on the Linux command line\n",
    "\n",
    "### Data-oriented Programming Paradigms,  2021 WS\n",
    "10/12/2021\n",
    "\n",
    "GÃ¡bor Recski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we learn how to perform simple text processing tasks by combining a set of tools available on __UNIX-like systems__ (such as Linux and Mac OS) using __pipes__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a very brief introduction to the Linux command line, including links to additional documentation, see this notebook:\n",
    "\n",
    "\n",
    "[Introduction to the Linux command line](https://github.com/tuw-nlp-ie/tuw-nlp-ie-2021WS/blob/main/lectures/01_Text_processing/01a_Intro_to_Linux_command_line.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the commands inside this notebook, you need to install the bash kernel for jupyter, e.g. like this:\n",
    "```\n",
    "pip install bash-kernel\n",
    "python -m bash_kernel.install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps are based on two files in the `data` folder, `alice_tok.txt` contains the **tokenized** version of the novel _Alice in Wonderland_ and `data/stopwords.txt` contains a list of English **stopwords**, words that express some grammatical function that we often want to ignore in text processing applications. Both files were created in [this notebook](https://github.com/tuw-nlp-ie/tuw-nlp-ie-2021WS/blob/main/lectures/01_Text_processing/01_Text_processing.ipynb) on the basics of text processsing.\n",
    "\n",
    "Let's have a look at the two files we are going to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/alice_tok.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/stopwords.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`grep` is the command for matching regular expressions. Let's use it to find capitalized words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipe symbol `|` means that the output of one command becomes the input of the next one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grep -E '^[A-Z][a-z]+' data/alice_tok.txt | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_You can ignore the occasional `Broken pipe` errors, it just means that a command in the pipeline was still writing output when the next one was already finished_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cat` command is often used at the beginning of a pipe, since all it does by default is send the contents of the file to the standard output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat data/alice_tok.txt | grep -E '^[A-Z][a-z]+' | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting can be implemented as a combination of sorting and aggregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat data/alice_tok.txt | grep -E '^[A-Z][a-z]+' | sort | uniq -c | sort -nr | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this for later. The `sed` command can be used for regex-based search-and-replace, here we use it to get a more convenient format. Then we sort the lines alphabetically, later we'll see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat data/alice_tok.txt | grep -E '^[A-Z][a-z]+' | sort | uniq -c | sort -nr | sed 's/^ *\\([0-9]*\\) \\(.*\\)$/\\2\\t\\1/' | sort | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat data/alice_tok.txt | grep -E '^[A-Z][a-z]+' | sort | uniq -c | sort -nr | sed 's/^ *\\([0-9]*\\) \\(.*\\)$/\\2\\t\\1/' | sort > data/ent_freqs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat data/alice_tok.txt | grep -E '^[A-Z][a-z]+' | sort | uniq -c | sort -nr | head -50 | sed 's/^[ 0-9]*//' | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat data/alice_tok.txt | grep -E '^[A-Z][a-z]+' | sort | uniq -c | sort -nr | head -50 | sed 's/^[ 0-9]*//' | sort | tr [:upper:] [:lower:] | comm -13 data/stopwords.txt - | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find a way to get rid of the first words of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat data/alice_tok.txt | sed 's/^$/@/' | tr '\\n' ' ' | sed 's/ @ /\\n/g' | cut -d' ' -f2- | tr ' ' '\\n' | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat data/alice_tok.txt | sed 's/^$/@/' | tr '\\n' ' ' | sed 's/ @ /\\n/g' | cut -d' ' -f2- | tr ' ' '\\n' | grep -E '^[A-Z][a-z]+' | sort | uniq -c | sort -nr | head -50 | sed 's/^[ 0-9]*//' | sort | tr [:upper:] [:lower:] | comm -13 data/stopwords.txt - | head -30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the frequencies from the saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat data/alice_tok.txt | sed 's/^$/@/' | tr '\\n' ' ' | sed 's/ @ /\\n/g' | cut -d' ' -f2- | tr ' ' '\\n' | grep -E '^[A-Z][a-z]+' | sort | uniq -c | sort -nr | head -50 | sed 's/^[ 0-9]*//' | sort | tr [:upper:] [:lower:] | comm -13 data/stopwords.txt - | sed 's/^./\\u&/' | join - data/ent_freqs.txt | sort -k2 -nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. (25 points) Redo all steps of extracting a frequency count of entities, but using _Alice in Wonderland_ in another language. The German version is in `data/alice_de.txt`, but you can choose any other language for which you can find a plain text version online (try [Project Gutenberg](https://www.gutenberg.org/))! Start by adapting the preprocessing and segmentation steps in [this notebook](https://github.com/tuw-nlp-ie/tuw-nlp-ie-2021WS/blob/main/lectures/01_Text_processing/01_Text_processing.ipynb) to your chosen language and creating the two files used in this notebook (the tokenized text and the list of stopwords). Then check if the remaining steps also need modification.\n",
    "\n",
    "1. (75 points) Improve the solution in this notebook to also include multi-word entities (we didn't find the Mock Turtle or the March Hare!). There may be many different ways to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission instructions\n",
    "\n",
    "Submit your solution via TUWEL, by uploading 3 files:\n",
    "- The tokenized input text for your chosen language (e.g. `alice_de_tok.txt`)\n",
    "- The list of stopwords for your chosen language (e.g. `stopwords_de.txt`)\n",
    "- A file with the extension `.sh` containing your command(s), with short explanations as comments (lines preceded by #)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The cell below shows how the solution in this notebook would have to be submitted.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract capitalized words, count them, reformat, save to file: \n",
    "cat data/alice_tok.txt | grep -E '^[A-Z][a-z]+' | sort | uniq -c | sort -nr | sed 's/^ *\\([0-9]*\\) \\(.*\\)$/\\2\\t\\1/' | sort > data/ent_freqs.txt\n",
    "\n",
    "# reformat to get one sentence per line, keep all but the first words of sentences, reformat again to one word per line, extract capitalized words, count them, keep the top 50, and then only those that are not in the stopwords file, finally match the lines to the lines of the word frequency file and sort by frequency\n",
    "cat data/alice_tok.txt | sed 's/^$/@/' | tr '\\n' ' ' | sed 's/ @ /\\n/g' | cut -d' ' -f2- | tr ' ' '\\n' | grep -E '^[A-Z][a-z]+' | sort | uniq -c | sort -nr | head -50 | sed 's/^[ 0-9]*//' | sort | tr [:upper:] [:lower:] | comm -13 data/stopwords.txt - | sed 's/^./\\u&/' | join - data/ent_freqs.txt | sort -k2 -nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd homework/ex1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(\"_\", \"\", text)\n",
    "    cleaned_text = re.sub(\"\\n\", \" \", cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def head(text, n: int = 1000):\n",
    "    print(text[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"data/alice_de.txt\").read()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean_text(text)\n",
    "head(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = sent_tokenize(text)\n",
    "head(sens, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = [word_tokenize(sen) for sen in sens]\n",
    "\n",
    "with open(\"data/alice_tok_de.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\\n\".join(\"\\n\".join(sen) for sen in toks) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words(\"german\"))\n",
    "\n",
    "# should all be lower case words\n",
    "print(list(stopwords)[0:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    if isinstance(iterable, set):\n",
    "      iterable = list(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "words_per_line = 7\n",
    "\n",
    "print(\"\\n\".join([\", \".join([word for word in w_slice]) for w_slice in batch(stopwords, words_per_line)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def check_if_in(wset, pattern, regex=False):\n",
    "    if regex:\n",
    "        res = re.search(pattern, '\\n'.join(wset))\n",
    "        print(f\"found: {res}\")\n",
    "\n",
    "    else:\n",
    "        if pattern in wset:\n",
    "            print(f\"found: {pattern}\")\n",
    "\n",
    "# check_if_in(stopwords, 'sie')\n",
    "check_if_in(stopwords, '[Ss]ie', regex=True)\n",
    "check_if_in(stopwords, '[Jj]a', regex=True)\n",
    "check_if_in(stopwords, '[Nn]ein', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think 'Ja' and 'Nein' would be good additions to \n",
    "# the german stopword list.\n",
    "# I noticed both words were still in the filtered entity/character list.\n",
    "# I researched other stoplists and realized that the nltk list is rather minimal.\n",
    "# One can always argue that if a character was named 'Ja', \n",
    "# which some authors might actually do,\n",
    "# it would get removed that way. Then again, you can say the same\n",
    "# about any word on the stopword list.\n",
    "stopwords.add('ja')\n",
    "stopwords.add('nein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the 'data'-folder doesn't exist, it needs to be created\n",
    "# or the cwd of the jupyterlab kernel instance needs to be adjusted\n",
    "with open(\"data/stopwords_de.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(sorted(stopwords)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Frequency count of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# extract capitalized words, count them, reformat, save to file: \n",
    "cat data/alice_tok_de.txt | grep -E '^[A-Z][a-z]+' | sort | uniq -c | sort -nr | sed 's/^ *\\([0-9]*\\) \\(.*\\)$/\\2\\t\\1/' | tr [:upper:] [:lower:] | sort > out/ent_freqs.txt\n",
    "\n",
    "# reformat to get one sentence per line, \n",
    "# get rid of the bracketed numbers: [ xx ] (e.g. [1] before 'Erstes Kapitel'),\n",
    "# keep all but the first words of sentences, reformat again to one word per line, \n",
    "# extract capitalized words, count them, keep the top 50, \n",
    "# and then only those that are not in the stopwords file, \n",
    "# finally match the lines to the lines of the word frequency file and sort by frequency\n",
    "cat data/alice_tok_de.txt | sed 's/^$/@/' | tr '\\n' ' ' | sed 's/ @ /\\n/g' | \\\n",
    "sed -E \"s/\\[ ?[0-9]+ ?\\] //\" | \\\n",
    "cut -d' ' -f2- | tr ' ' '\\n' | \\\n",
    "grep -E '^[A-Z][a-z]+' | sort | uniq -c | sort -nr | head -50 | \\\n",
    "sed 's/^[ 0-9]*//' | sort | tr [:upper:] [:lower:] | comm -13 data/stopwords_de.txt - | \\\n",
    "join - out/ent_freqs.txt | sort -k2 -nr > out/result_german.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Multi-word Entities\n",
    "\n",
    "(75 points) Improve the solution in this notebook to also include multi-word entities (we didn't find the Mock Turtle or the March Hare!). There may be many different ways to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9cbeccf2996891a32c7b61f0eadf4f9520d40da65cd0d64cc2707f8c29de0c83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('dopp-Qi_QSkxP-py3.9': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
